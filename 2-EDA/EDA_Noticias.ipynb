{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b94c12",
   "metadata": {},
   "source": [
    "\n",
    "# EDA de Noticias + Embeddings (multilingüe) + Agregaciones Diarias\n",
    "\n",
    "Este notebook está pensado para analizar el corpus de noticias obtenido del *scraper de La Nación* (secciones **Política** y **Economía**), generar **embeddings** con un modelo de **Hugging Face** en español/bilingüe, y calcular **agregaciones diarias** (conteos y vectores promedio por día).\n",
    "\n",
    "> **Requisitos**\n",
    "> - Python 3.10+\n",
    "> - `pandas`, `numpy`, `matplotlib`\n",
    "> - `sentence-transformers` (para embeddings de oraciones)\n",
    ">\n",
    "> **Entrada esperada**: un archivo `.parquet` o `.csv` con al menos estas columnas:\n",
    "> - `id`\n",
    "> - `seccion`\n",
    "> - `fecha_publicacion` (ISO 8601)\n",
    "> - `titulo`\n",
    "> - `url`\n",
    "> - `bajada` (opcional)\n",
    "> - `texto` (opcional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1eeca3",
   "metadata": {},
   "source": [
    "## 1) Instalación\n",
    "\n",
    "Segun requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejecuta esta celda solo si te faltan dependencias\n",
    "# (Comenta lo que ya tengas instalado)\n",
    "# Nota: usa un entorno virtual (venv/conda) antes de instalar.\n",
    "\n",
    "# %pip install --upgrade pip\n",
    "%pip install pandas numpy matplotlib sentence-transformers pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cdb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ef7bfc",
   "metadata": {},
   "source": [
    "## 2) Imports y configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13bf1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Opciones de pandas para display\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# Helper para plots (sin seaborn, solo matplotlib)\n",
    "def lineplot(x, y, title=None, xlabel=None, ylabel=None):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(x, y)\n",
    "    if title: plt.title(title)\n",
    "    if xlabel: plt.xlabel(xlabel)\n",
    "    if ylabel: plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def imshow_matrix(mat, xticks=None, yticks=None, title=None, xlabel=None, ylabel=None):\n",
    "    plt.figure(figsize=(7,6))\n",
    "    im = plt.imshow(mat, aspect='auto')\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    if title: plt.title(title)\n",
    "    if xlabel: plt.xlabel(xlabel)\n",
    "    if ylabel: plt.ylabel(ylabel)\n",
    "    if xticks is not None:\n",
    "        plt.xticks(range(len(xticks)), xticks, rotation=90)\n",
    "    if yticks is not None:\n",
    "        plt.yticks(range(len(yticks)), yticks)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def to_date_only(s):\n",
    "    # Convierte ISO 8601 a fecha (YYYY-MM-DD)\n",
    "    try:\n",
    "        return pd.to_datetime(s, utc=True).date()\n",
    "    except Exception:\n",
    "        # Intentos alternativos\n",
    "        try:\n",
    "            return pd.to_datetime(s).date()\n",
    "        except Exception:\n",
    "            return pd.NaT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec067e",
   "metadata": {},
   "source": [
    "## 3) Cargar dataset de noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2040642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ajusta la ruta al archivo generado por el scraper\n",
    "# Ejemplo: noticias_LN_2025Q1.parquet\n",
    "DATA_PATH = Path(\"./noticias_LN_2025Q1.parquet\")\n",
    "\n",
    "if DATA_PATH.suffix.lower() == \".parquet\":\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "else:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Dimensiones:\", df.shape)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe08a6",
   "metadata": {},
   "source": [
    "## 4) Limpieza mínima y normalización de campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6beac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aseguramos columnas esperadas\n",
    "expected_cols = ['id','seccion','fecha_publicacion','titulo','url']\n",
    "for c in expected_cols:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Falta la columna requerida: {c}\")\n",
    "\n",
    "# Texto base: preferimos `texto` si existe; si no, construimos a partir de `titulo` + `bajada`\n",
    "if 'texto' in df.columns and df['texto'].notna().any():\n",
    "    df['texto_base'] = df['texto'].fillna('')\n",
    "else:\n",
    "    df['texto_base'] = ''\n",
    "    if 'titulo' in df.columns:\n",
    "        df['texto_base'] = df['texto_base'] + df['titulo'].fillna('')\n",
    "    if 'bajada' in df.columns:\n",
    "        df['texto_base'] = (df['texto_base'] + ' ' + df['bajada'].fillna('')).str.strip()\n",
    "\n",
    "# Normalizamos fecha a día\n",
    "df['fecha_dia'] = df['fecha_publicacion'].apply(to_date_only)\n",
    "df = df.dropna(subset=['fecha_dia'])\n",
    "\n",
    "# Longitud de textos (caracteres y palabras aprox)\n",
    "df['len_chars'] = df['texto_base'].fillna('').str.len()\n",
    "df['len_words'] = df['texto_base'].fillna('').str.split().apply(len)\n",
    "\n",
    "df[['id','seccion','fecha_dia','len_chars','len_words']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52141e13",
   "metadata": {},
   "source": [
    "## 5) EDA básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05525d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conteo por sección\n",
    "conteo_seccion = df['seccion'].value_counts().sort_values(ascending=False)\n",
    "display(conteo_seccion)\n",
    "\n",
    "# Conteo diario\n",
    "conteo_diario = df.groupby('fecha_dia')['id'].nunique().sort_index()\n",
    "display(conteo_diario.head(10))\n",
    "\n",
    "# Serie temporal de notas por día\n",
    "lineplot(conteo_diario.index, conteo_diario.values, \n",
    "         title=\"Cantidad de noticias por día\", xlabel=\"Fecha\", ylabel=\"Notas/día\")\n",
    "\n",
    "# Distribución de longitudes (caracteres)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(df['len_chars'], bins=40)\n",
    "plt.title(\"Distribución de longitudes de texto (caracteres)\")\n",
    "plt.xlabel(\"Caracteres\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb04037",
   "metadata": {},
   "source": [
    "## 6) Embeddings (Hugging Face, multilingüe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e93155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Elegí uno de los modelos multilingües recomendados:\n",
    "# - 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  (ligero y bueno)\n",
    "# - 'distiluse-base-multilingual-cased-v1' (ligero y popular)\n",
    "\n",
    "MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# Texto a embebeder\n",
    "texts = df['texto_base'].fillna('').tolist()\n",
    "\n",
    "# Batch encode (ajustá batch_size si es necesario)\n",
    "embeddings = model.encode(texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "emb_dim = embeddings.shape[1]\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f9b4a",
   "metadata": {},
   "source": [
    "### 6.1) (Opcional) Guardar embeddings por noticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Guardamos en un parquet separado (vector como lista)\n",
    "out_news_emb = Path('./news_embeddings.parquet')\n",
    "df_emb = pd.DataFrame({\n",
    "    'id': df['id'].values,\n",
    "    'fecha_dia': df['fecha_dia'].astype(str).values,\n",
    "    'seccion': df['seccion'].values,\n",
    "    'url': df['url'].values,\n",
    "    'titulo': df.get('titulo', pd.Series(['']*len(df))).values,\n",
    "    'embedding': [emb.tolist() for emb in embeddings]\n",
    "})\n",
    "df_emb.to_parquet(out_news_emb, index=False)\n",
    "print(f\"Guardado: {out_news_emb} ({len(df_emb)} filas)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45a536",
   "metadata": {},
   "source": [
    "## 7) Agregación diaria de embeddings (promedio por día)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Construimos matriz día x embedding (promedio)\n",
    "df['__row'] = range(len(df))  # índice para recuperar luego\n",
    "df_day_groups = df.groupby('fecha_dia').indices  # dict: fecha -> indices de filas\n",
    "\n",
    "fechas = sorted(list(df_day_groups.keys()))\n",
    "day_vectors = []\n",
    "day_counts = []\n",
    "\n",
    "for fecha in fechas:\n",
    "    idxs = df_day_groups[fecha]\n",
    "    vecs = embeddings[idxs]\n",
    "    day_vectors.append(vecs.mean(axis=0))\n",
    "    day_counts.append(len(idxs))\n",
    "\n",
    "day_vectors = np.vstack(day_vectors)  # shape: (n_dias, emb_dim)\n",
    "day_counts = np.array(day_counts)\n",
    "\n",
    "print(\"Matriz diaria:\", day_vectors.shape)\n",
    "print(\"Conteos por día (primeros 10):\", day_counts[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54043f7d",
   "metadata": {},
   "source": [
    "## 8) Visualizaciones sobre agregaciones diarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eabdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PCA a 2D para visualizar evolución diaria\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "xy = pca.fit_transform(day_vectors)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(xy[:,0], xy[:,1], s=np.clip(day_counts, 10, 120))\n",
    "for i, f in enumerate(fechas):\n",
    "    plt.text(xy[i,0], xy[i,1], str(f), fontsize=8)\n",
    "\n",
    "plt.title(\"Embeddings diarios (PCA 2D) - tamaño ~ cantidad de notas\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Similaridad coseno entre días\n",
    "sim = cosine_similarity(day_vectors)\n",
    "imshow_matrix(sim, xticks=[str(f) for f in fechas], yticks=[str(f) for f in fechas],\n",
    "              title=\"Similaridad coseno entre días\", xlabel=\"Días\", ylabel=\"Días\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9f623",
   "metadata": {},
   "source": [
    "## 9) (Opcional) Embeddings por sección y día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458762cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Promedio por (fecha_dia, seccion)\n",
    "pairs = df.groupby(['fecha_dia','seccion']).indices\n",
    "pairs_keys = sorted(list(pairs.keys()))\n",
    "\n",
    "pair_vectors = []\n",
    "for k in pairs_keys:\n",
    "    idxs = pairs[k]\n",
    "    pair_vectors.append(embeddings[idxs].mean(axis=0))\n",
    "\n",
    "pair_vectors = np.vstack(pair_vectors)\n",
    "\n",
    "# Proyección PCA para ver si hay separación por secciones a lo largo del tiempo\n",
    "xy2 = PCA(n_components=2, random_state=42).fit_transform(pair_vectors)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for (f, s), (x, y) in zip(pairs_keys, xy2):\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x, y, f\"{f}-{s}\", fontsize=8)\n",
    "plt.title(\"Embeddings promedio por (día, sección)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bd064",
   "metadata": {},
   "source": [
    "## 10) Export de agregaciones diarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Guardamos los vectores diarios y conteos\n",
    "daily_df = pd.DataFrame({\n",
    "    'fecha_dia': [str(f) for f in fechas],\n",
    "    'count_noticias': day_counts,\n",
    "    'embedding': [v.tolist() for v in day_vectors]\n",
    "})\n",
    "out_daily = Path('./daily_news_embeddings.parquet')\n",
    "daily_df.to_parquet(out_daily, index=False)\n",
    "print(f\"Guardado: {out_daily} ({len(daily_df)} filas)\")\n",
    "\n",
    "# CSV con conteos diarios (sin embeddings)\n",
    "conteo_diario = pd.Series(day_counts, index=pd.Index(fechas, name='fecha_dia'), name='count_noticias')\n",
    "conteo_csv = Path('./daily_news_counts.csv')\n",
    "conteo_diario.to_csv(conteo_csv, header=True)\n",
    "print(f\"Guardado: {conteo_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921229fe",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Checklist / Próximos pasos\n",
    "\n",
    "- [ ] Añadir **detección de idioma** para métricas (es/en) por día (fasttext-langdetect o spacy-cld3).\n",
    "- [ ] Probar otros modelos de embeddings (por ejemplo, `intfloat/multilingual-e5-base` con formato de prompt apropiado).\n",
    "- [ ] Construir **índice vectorial** (FAISS) para búsquedas por similitud entre días o noticias.\n",
    "- [ ] Agregar **topic modeling** (BERTopic o LDA clásico como línea base).\n",
    "- [ ] Incorporar **sentimiento** por día/sección y correlacionarlo con los ejes PCA.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
