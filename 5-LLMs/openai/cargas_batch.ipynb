{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27338ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PIPELINE BATCH PARA EXTRACCIÓN ESTRUCTURADA\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68dbc5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydantic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Literal\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydantic'"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. LIBRERÍAS.\n",
    "# -----------------------------\n",
    "from __future__ import annotations\n",
    "from typing import List, Optional, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "import json, pathlib\n",
    "from openai import OpenAI\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0be85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. CONSTANTES.\n",
    "# -----------------------------\n",
    "project_path = \"C:/Users/i_link/Maestría/Text Mining/nlp_dmuba/\"\n",
    "dataset_file_path = project_path + \"1-Scraping/dataset_consolidado/df.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65600df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. LECTURA DE DATOS.\n",
    "# -----------------------------\n",
    "df = pd.read_parquet(dataset_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d115f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: sample para pruebas\n",
    "sample = 50\n",
    "df_sample = df.dropna(subset=[\"contenido\"]).sample(sample, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. LECTURA DE CLAVE OPENAI.\n",
    "# -----------------------------\n",
    "with open(project_path + \".secrets/openai_api_key.txt\", \"r\") as f:\n",
    "    key = f.read().strip()\n",
    "\n",
    "client = OpenAI(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98db2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. PROMPTS Y SYSTEM.\n",
    "# -----------------------------\n",
    "SYSTEM_PROMPT = '''\n",
    "Eres un analista económico-financiero especializado en Argentina.\n",
    "Objetivo: extraer datos ESTRUCTURADOS de una noticia para modelar el MERVAL.\n",
    "Reglas de oro:\n",
    "- Usa SOLO el texto de la noticia.\n",
    "- Si no hay evidencia clara: usa 0.0, \"unknown\" o null.\n",
    "- Valores en [-1..1]; confianza y calidad en [0..1].\n",
    "- horizonte_dias SOLO si se menciona explícitamente.\n",
    "- Listas sin duplicados; tickers en MAYÚSCULAS.\n",
    "'''\n",
    "\n",
    "USER_TEMPLATE = '''\n",
    "Diario: {diario}\n",
    "Fecha: {fecha}\n",
    "Seccion: {seccion}\n",
    "Titulo: {titulo}\n",
    "Contenido: {contenido}\n",
    "\n",
    "Devuelve SOLO el JSON con el esquema pedido.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. CREAR ARCHIVO JSONL PARA BATCH.\n",
    "# -----------------------------\n",
    "batch_requests_path = pathlib.Path(project_path) / \"batch_requests.jsonl\"\n",
    "\n",
    "with open(batch_requests_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, row in df_sample.iterrows():\n",
    "        contenido = (row.get(\"contenido\") or \"\")[:8000]\n",
    "        prompt = USER_TEMPLATE.format(\n",
    "            diario=row.get(\"diario\", \"unknown\"),\n",
    "            fecha=str(row.get(\"fecha\", \"unknown\")),\n",
    "            seccion=row.get(\"seccion\", \"unknown\"),\n",
    "            titulo=row.get(\"titulo\", \"unknown\"),\n",
    "            contenido=contenido\n",
    "        )\n",
    "\n",
    "        request_dict = {\n",
    "            \"custom_id\": f\"row_{i}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/responses\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-5-mini\",\n",
    "                \"input\": [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"text_format\": \"json\"  # devuelve JSON válido\n",
    "            }\n",
    "        }\n",
    "        f.write(json.dumps(request_dict, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Archivo JSONL creado en {batch_requests_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. SUBIR EL BATCH.\n",
    "# -----------------------------\n",
    "batch_job = client.batches.create(\n",
    "    input_file=open(batch_requests_path, \"rb\"),\n",
    "    endpoint=\"/v1/responses\",\n",
    "    completion_window=\"24h\",  # OpenAI puede tardar hasta 24h según volumen.\n",
    ")\n",
    "print(\"Batch job creado:\", batch_job.id)\n",
    "print(\"Status inicial:\", batch_job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. DESCARGAR RESULTADOS (cuando esté listo)\n",
    "# -----------------------------\n",
    "# status = client.batches.retrieve(batch_job.id)\n",
    "# result_file = client.files.retrieve_content(status.output_file_id)\n",
    "# with open(project_path + \"batch_results.jsonl\", \"wb\") as f_out:\n",
    "#     f_out.write(result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713af00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 9. PARSEAR JSONL Y UNIR CON DF\n",
    "# -----------------------------\n",
    "# df_results = []\n",
    "# with open(project_path + \"batch_results.jsonl\", \"r\", encoding=\"utf-8\") as f_in:\n",
    "#     for line in f_in:\n",
    "#         data = json.loads(line)\n",
    "#         df_results.append(data.get(\"output\", {}))\n",
    "# df_features = pd.json_normalize(df_results, sep=\"__\")\n",
    "# df_final = pd.concat([df_sample.reset_index(drop=True), df_features], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruebas_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
